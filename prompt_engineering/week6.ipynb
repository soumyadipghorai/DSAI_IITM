{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1PKCwCyomixj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PKCwCyomixj",
        "outputId": "c4b02ebb-1771-48dc-c401-b541718f75c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-1.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.1.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.9.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.53.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.53.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.12.3)\n",
            "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.12.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.1->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.1->langchain-openai) (0.4.55)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.1->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.1->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.1->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.1->langchain-openai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.1->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.1->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.53.0->langchain-google-genai) (0.6.1)\n",
            "Downloading langchain_groq-1.1.0-py3-none-any.whl (19 kB)\n",
            "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.37.1 langchain-groq-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-openai langchain-google-genai langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "IaCBWiT7l85F",
      "metadata": {
        "id": "IaCBWiT7l85F"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "yiVWkyApl1ZD",
      "metadata": {
        "id": "yiVWkyApl1ZD"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "llm = init_chat_model(model=\"openai/gpt-oss-120b\", model_provider=\"groq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "keOYHdLfl1Tn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "keOYHdLfl1Tn",
        "outputId": "0c5f3853-045b-4cfb-f8a6-9ca26267ba51"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The capital of India is **New Delhi**."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(llm.invoke('what is the capital of india').content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "vQmJWv2tl1MF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vQmJWv2tl1MF",
        "outputId": "465a667f-e559-4056-aeb3-12db89e532d6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain is a framework that lets developers build LLM‑powered applications by chaining together language model calls, data sources, and tools in a modular, programmable workflow."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**LLM = Large Language Model**\n",
              "\n",
              "A *large language model* (LLM) is a type of artificial‑intelligence system that has been trained on massive amounts of text data so that it can understand and generate human‑like language. Here’s a breakdown of what that means:\n",
              "\n",
              "---\n",
              "\n",
              "## 1. Core Idea\n",
              "- **“Language model”**: A statistical model that predicts the next word (or token) in a sequence given the words that came before it. By repeatedly making these predictions, it can produce coherent sentences, answer questions, translate text, and more.\n",
              "- **“Large”**: Refers to two things:\n",
              "  1. **Scale of data** – billions to trillions of words from books, articles, webpages, code, etc.\n",
              "  2. **Scale of the model** – millions to billions (or even trillions) of parameters, which are the internal numeric weights the model learns during training.\n",
              "\n",
              "---\n",
              "\n",
              "## 2. How LLMs Are Built\n",
              "\n",
              "| Step | What Happens |\n",
              "|------|--------------|\n",
              "| **Data collection** | Gather a huge, diverse corpus of text (e.g., web crawls, books, scientific papers). |\n",
              "| **Tokenization** | Break the text into manageable units (words, sub‑words, or characters) called *tokens*. |\n",
              "| **Model architecture** | Most modern LLMs use the **Transformer** architecture (introduced in 2017). It relies on self‑attention mechanisms that let the model weigh the relevance of each token to every other token. |\n",
              "| **Training** | The model learns by trying to predict masked or next tokens and adjusting its parameters to reduce prediction error. This is done on powerful GPUs/TPUs for days or weeks. |\n",
              "| **Fine‑tuning (optional)** | After the general training, the model can be further trained on a narrower dataset or with reinforcement learning from human feedback (RLHF) to make it safer or more task‑specific. |\n",
              "\n",
              "---\n",
              "\n",
              "## 3. What LLMs Can Do\n",
              "\n",
              "- **Text generation** – write essays, stories, code snippets, poetry, etc.\n",
              "- **Question answering** – retrieve or synthesize information from its training knowledge.\n",
              "- **Summarization** – condense long documents into shorter abstracts.\n",
              "- **Translation** – convert text between languages.\n",
              "- **Conversation** – act as a chatbot or virtual assistant.\n",
              "- **Classification & extraction** – label sentiment, pull out entities, detect topics.\n",
              "- **Code assistance** – autocomplete, debug, or explain programming code.\n",
              "\n",
              "---\n",
              "\n",
              "## 4. Why “Large” Matters\n",
              "\n",
              "- **Pattern capture**: Bigger models with more parameters can learn subtler statistical regularities, idioms, reasoning steps, and even some world knowledge.\n",
              "- **Generalization**: With enough data, LLMs can perform tasks they were never explicitly trained for (zero‑shot or few‑shot learning).\n",
              "- **Quality vs. cost**: Larger models tend to produce higher‑quality output but require more compute, memory, and energy, making them expensive to train and run.\n",
              "\n",
              "---\n",
              "\n",
              "## 5. Limitations & Risks\n",
              "\n",
              "| Issue | Explanation |\n",
              "|-------|-------------|\n",
              "| **Hallucination** | The model may generate plausible‑looking but factually incorrect statements. |\n",
              "| **Bias** | It inherits biases present in its training data (gender, racial, cultural, etc.). |\n",
              "| **Data privacy** | If training data contains personal or copyrighted material, there are legal/ethical concerns. |\n",
              "| **Resource intensive** | Training and even inference can consume large amounts of electricity and specialized hardware. |\n",
              "| **Interpretability** | Understanding why a model made a specific prediction is still an open research problem. |\n",
              "\n",
              "---\n",
              "\n",
              "## 6. Popular LLM Families (as of 2025)\n",
              "\n",
              "| Model | Parameters | Notable Traits |\n",
              "|-------|------------|----------------|\n",
              "| **GPT‑4** (OpenAI) | ~1‑2 trillion (estimated) | Strong reasoning, multimodal (text+image). |\n",
              "| **Claude** (Anthropic) | ~1 trillion | Emphasizes “constitutional” safety prompts. |\n",
              "| **LLaMA 2** (Meta) | 7 B – 70 B | Open‑source weights, widely used for research. |\n",
              "| **Gemma** (Google DeepMind) | 2 B – 7 B | Optimized for efficiency on consumer hardware. |\n",
              "| **Mistral** (Mistral AI) | 7 B – 13 B | Focus on instruction‑following and fine‑tuning. |\n",
              "\n",
              "---\n",
              "\n",
              "## 7. Quick Analogy\n",
              "\n",
              "Think of an LLM like a **very large, highly experienced autocomplete**. When you type a few words, it draws on everything it has “read” to guess the most likely continuation, but it also has learned patterns that let it *reason* about what you might want—much like a seasoned writer who can finish a story from a single opening line.\n",
              "\n",
              "---\n",
              "\n",
              "## 8. Bottom‑Line Summary\n",
              "\n",
              "- **LLM = Large Language Model** → a neural network trained on massive text corpora.\n",
              "- It predicts the next token, enabling it to generate and understand language.\n",
              "- Size (data + parameters) gives it impressive capabilities, but also brings cost, bias, and reliability challenges.\n",
              "- LLMs are now foundational tools for chatbots, writing assistants, code helpers, translation services, and many other AI‑driven applications."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "### What a Vector Embedding Is\n",
              "\n",
              "A **vector embedding** (often just called an *embedding*) is a way of representing a piece of data—such as a word, a sentence, an image, a user, or even a graph node—as a point in a continuous, high‑dimensional numeric space. In that space, the geometry (distances and directions) captures the relationships and similarities that are important for the task at hand.\n",
              "\n",
              "In other words, an embedding is a **dense, fixed‑length vector of real numbers** that encodes the meaning, structure, or behavior of the original object in a form that machine‑learning models can easily work with.\n",
              "\n",
              "---\n",
              "\n",
              "## 1. Why Do We Need Embeddings?\n",
              "\n",
              "| Traditional Representation | Embedding Representation |\n",
              "|----------------------------|--------------------------|\n",
              "| **Sparse / high‑dimensional** (e.g., one‑hot vectors for words: 1 in a 50 000‑dimensional vector, 0 elsewhere) | **Dense / low‑dimensional** (e.g., 300‑dimensional real‑valued vector) |\n",
              "| No notion of similarity—two different words are orthogonal | Similar items are *close* (small Euclidean / cosine distance) |\n",
              "| Hard for models to learn patterns from raw high‑dim data | Enables efficient learning, generalization, and transfer across tasks |\n",
              "\n",
              "Embeddings turn raw, discrete symbols into a smooth numeric landscape where “nearby” points mean “semantically or structurally similar”.\n",
              "\n",
              "---\n",
              "\n",
              "## 2. How Are Embeddings Obtained?\n",
              "\n",
              "### 2.1. **Learning from Data (Data‑Driven Embeddings)**  \n",
              "Most modern embeddings are **learned** by optimizing a neural network on a large corpus of examples.\n",
              "\n",
              "| Method | Typical Domain | Core Idea |\n",
              "|--------|----------------|-----------|\n",
              "| **Word2Vec (CBOW / Skip‑gram)** | Text | Predict a word from its context (or vice‑versa). Words that appear in similar contexts get similar vectors. |\n",
              "| **GloVe** | Text | Factorize a word‑co‑occurrence matrix; the dot product of two word vectors approximates their log‑co‑occurrence probability. |\n",
              "| **FastText** | Text | Like Word2Vec but also uses sub‑word (character n‑gram) information, giving better representations for rare or misspelled words. |\n",
              "| **BERT / RoBERTa / GPT** | Text (sentence / token level) | Train deep Transformers on masked‑language‑model or autoregressive objectives; the hidden states become contextual embeddings. |\n",
              "| **DeepWalk / node2vec** | Graphs | Perform random walks on a graph, treat walks like sentences, and apply Word2Vec to learn node embeddings. |\n",
              "| **ResNet / Vision Transformers** | Images | Pass an image through a CNN or ViT; the activation of a penultimate layer (often after a pooling step) is the image embedding. |\n",
              "| **Autoencoders / Variational Autoencoders** | Any modality | Encode data into a bottleneck layer (the latent vector) and decode it back; the bottleneck learns a compact embedding. |\n",
              "| **Metric‑learning losses (Triplet, Contrastive)** | Faces, product recommendations, etc. | Force embeddings of similar items to be close and dissimilar items to be far apart. |\n",
              "\n",
              "### 2.2. **Hand‑Crafted / Pre‑Defined Embeddings**  \n",
              "Before deep learning, people sometimes built embeddings manually:\n",
              "\n",
              "* **One‑hot vectors** (very sparse, no similarity information).  \n",
              "* **TF‑IDF vectors** (sparse, weighted word counts).  \n",
              "* **Principal Component Analysis (PCA) / SVD** of co‑occurrence matrices.\n",
              "\n",
              "These are still useful in low‑resource settings but are far less expressive than learned dense embeddings.\n",
              "\n",
              "---\n",
              "\n",
              "## 3. What Does an Embedding Look Like?\n",
              "\n",
              "A concrete example: the word **“king”** might be represented (after training a 300‑dimensional Word2Vec model) as:\n",
              "\n",
              "```\n",
              "[ 0.215, -0.134, 0.487, ..., -0.021 ]   (300 numbers)\n",
              "```\n",
              "\n",
              "Two important properties:\n",
              "\n",
              "* **Dimensionality** – The length of the vector (e.g., 50, 128, 768, 1024) is a hyper‑parameter. Higher dimensions can capture more nuance but risk over‑fitting and higher computational cost.\n",
              "* **Semantic arithmetic** – Because the space is learned, simple vector algebra often reveals relationships:\n",
              "  ```\n",
              "  embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") ≈ embedding(\"queen\")\n",
              "  ```\n",
              "\n",
              "---\n",
              "\n",
              "## 4. How Do We Use Embeddings?\n",
              "\n",
              "| Task | How Embeddings Help |\n",
              "|------|---------------------|\n",
              "| **Similarity / Retrieval** | Compute cosine similarity between vectors to find “nearest neighbors” (e.g., search for similar documents, images, or users). |\n",
              "| **Classification** | Feed embeddings as input features to a downstream classifier (e.g., sentiment analysis from sentence embeddings). |\n",
              "| **Clustering** | Group items by distance in embedding space (e.g., topic discovery, product segmentation). |\n",
              "| **Transfer Learning** | Re‑use a pre‑trained embedding (e.g., BERT) as a starting point for a new task, drastically reducing required labeled data. |\n",
              "| **Recommendation** | Represent users and items in the same space; recommend items whose vectors are close to a user’s vector. |\n",
              "| **Graph Analysis** | Node embeddings enable link prediction, community detection, and graph‑based recommendation. |\n",
              "\n",
              "---\n",
              "\n",
              "## 5. Key Intuitions & Analogies\n",
              "\n",
              "1. **Map Analogy** – Think of each object as a city. A raw one‑hot representation tells you only the city’s name, not where it is on a map. An embedding gives you latitude/longitude (coordinates). Nearby cities (vectors) are likely to share cultural, economic, or geographic traits.\n",
              "\n",
              "2. **Language Translation** – If you train separate embeddings for English and French words on comparable corpora, you can learn a linear transformation that aligns the two spaces. This reveals that the geometry of meaning is language‑agnostic.\n",
              "\n",
              "3. **Compress‑and‑Explain** – Embeddings are a compressed summary that keeps the “essence” needed for the downstream task, discarding irrelevant details (e.g., exact pixel values of an image after a few convolutional layers).\n",
              "\n",
              "---\n",
              "\n",
              "## 6. Practical Tips\n",
              "\n",
              "| Situation | Recommendation |\n",
              "|-----------|----------------|\n",
              "| **Small dataset** | Use a pre‑trained embedding (e.g., GloVe, FastText, BERT) and fine‑tune only a small head on top. |\n",
              "| **Domain‑specific vocabulary** | Fine‑tune a language model on your domain text, or train FastText on your own corpus to capture rare terms. |\n",
              "| **Memory constraints** | Choose a lower‑dimensional embedding (e.g., 64‑dim) or apply dimensionality reduction (PCA, quantization). |\n",
              "| **Need for interpretability** | Use techniques like **t‑SNE** or **UMAP** to visualize the embedding space; inspect nearest neighbors for sanity checks. |\n",
              "| **Cross‑modal tasks** (e.g., image‑text retrieval) | Align embeddings from different modalities into a shared space using contrastive learning (e.g., CLIP). |\n",
              "\n",
              "---\n",
              "\n",
              "## 7. Common Pitfalls\n",
              "\n",
              "1. **Assuming Euclidean distance is always the right metric** – For many embeddings, **cosine similarity** works better because magnitude differences are less meaningful.\n",
              "2. **Over‑relying on a single embedding** – Different models capture different aspects (syntax vs. semantics, local vs. global context). Ensemble or multi‑view embeddings can improve robustness.\n",
              "3. **Ignoring bias** – Embeddings inherit biases present in the training data (gender, racial, cultural). Use debiasing techniques (e.g., projection onto a neutral subspace) when fairness matters.\n",
              "4. **Treating embeddings as static** – Contextual models (BERT, GPT) produce **different vectors for the same word** depending on surrounding text. Make sure you extract the appropriate token or sentence representation.\n",
              "\n",
              "---\n",
              "\n",
              "## 8. A Minimal Code Sketch (Python)\n",
              "\n",
              "Below is a tiny example using the popular `sentence‑transformers` library to obtain a **sentence embedding**:\n",
              "\n",
              "```python\n",
              "# pip install sentence-transformers\n",
              "from sentence_transformers import SentenceTransformer, util\n",
              "\n",
              "model = SentenceTransformer('all-MiniLM-L6-v2')   # ~384‑dim embeddings, fast & small\n",
              "\n",
              "sentences = [\n",
              "    \"The cat sat on the mat.\",\n",
              "    \"A feline was lounging on a rug.\"\n",
              "]\n",
              "\n",
              "embeddings = model.encode(sentences, normalize_embeddings=True)  # shape (2, 384)\n",
              "\n",
              "# Cosine similarity (since vectors are already normalized)\n",
              "similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
              "print(f\"Similarity: {similarity.item():.4f}\")   # ~0.85 → they are semantically close\n",
              "```\n",
              "\n",
              "The `embeddings` variable now holds two dense vectors that you can store, index (e.g., with FAISS), or feed into downstream models.\n",
              "\n",
              "---\n",
              "\n",
              "## 9. TL;DR (One‑Sentence Summary)\n",
              "\n",
              "A vector embedding is a learned, dense numeric representation that maps discrete objects (words, images, users, etc.) into a continuous space where geometric closeness reflects semantic or structural similarity, enabling efficient similarity search, classification, and transfer learning across many AI tasks."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "inputs = [\n",
        "    'Explain langchain in one line',\n",
        "    'explain what is llm',\n",
        "    'explain what is vector embedding'\n",
        "]\n",
        "\n",
        "response = llm.batch(inputs)\n",
        "for res in response:\n",
        "    display(Markdown(res.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "kgTu13h-l1I3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "kgTu13h-l1I3",
        "outputId": "3ff531bd-c920-410a-83a9-40f6da625423"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The capital of India is **New Delhi**."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model_name = 'openai/gpt-oss-120b', temperature = 0.7)\n",
        "display(Markdown(llm.invoke('what is the capital of India').content))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dWPTGQfjlK9",
      "metadata": {
        "id": "1dWPTGQfjlK9"
      },
      "source": [
        "### Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "f0Ui3u6EjNfy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0Ui3u6EjNfy",
        "outputId": "06658596-30d5-4de4-8f58-649a6812b1c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['sentence', 'source_language', 'target_language'], input_types={}, partial_variables={}, template='Translate the following sentence from {source_language} to {target_language}: {sentence}')"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\n",
        "    \"Translate the following sentence from {source_language} to {target_language}: {sentence}\"\n",
        ")\n",
        "template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "q2WoEptTjtfk",
      "metadata": {
        "id": "q2WoEptTjtfk"
      },
      "outputs": [],
      "source": [
        "msg = template.invoke({'source_language': 'English', 'target_language': 'Bengali', 'sentence': 'how are you?'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "IDB_ZfHOky3_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "IDB_ZfHOky3_",
        "outputId": "d244b770-0427-4aac-ead3-053048e84681"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "আপনি কেমন আছেন?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(llm.invoke(msg).content))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7o-mENEBlMaN",
      "metadata": {
        "id": "7o-mENEBlMaN"
      },
      "source": [
        "### Output Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "emYIDx3DlDYd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "emYIDx3DlDYd",
        "outputId": "a7a4b302-2044-46fe-e331-8252f08db377"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'**Bengali translation:**  \\n- Formal / polite: **আপনি কেমন আছেন?**  \\n- Informal / casual: **তুমি কেমন আছো?**'"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "parser.invoke(llm.invoke(msg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "PZrp8ZQRlgoW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZrp8ZQRlgoW",
        "outputId": "eec04f72-8837-4ab2-83de-a70cbbc1edb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'give me one movie name and its release year',\n",
              " 'result': 'The Shawshank Redemption (1994)'}"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "llm = ChatGroq(model_name = 'openai/gpt-oss-120b', temperature = 0)\n",
        "\n",
        "class MyOutputSchema(BaseModel):\n",
        "    query: str = Field(description = 'User query')\n",
        "    result: str = Field(description=\"full llm generate response\")\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=MyOutputSchema)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['query'],\n",
        "    template = \"answer the following question. \\n {query} \\n {format_instructions}\"\n",
        ")\n",
        "\n",
        "filled_prompt = prompt.format(\n",
        "    query = 'give me one movie name and its release year',\n",
        "    format_instructions = parser.get_format_instructions()\n",
        ")\n",
        "\n",
        "response = llm.invoke(filled_prompt)\n",
        "structured_data = parser.invoke(response.content)\n",
        "structured_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ugi3JM4Wphy6",
      "metadata": {
        "id": "Ugi3JM4Wphy6"
      },
      "source": [
        "### Langchain Expression Language (LCEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "28gyrxj9mVq_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28gyrxj9mVq_",
        "outputId": "77091141-34f7-4928-e56b-9f2b4cc173c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'give me one movie name and its release year',\n",
              " 'result': 'The Shawshank Redemption (1994)'}"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = prompt | llm | parser\n",
        "\n",
        "# Run chain\n",
        "result = chain.invoke({\n",
        "    \"query\": \"give me one movie name and its release year\",\n",
        "    \"format_instructions\" : parser.get_format_instructions()\n",
        "})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sRYPPNbYqFmR",
      "metadata": {
        "id": "sRYPPNbYqFmR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
