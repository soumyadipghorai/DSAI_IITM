{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f010d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE \n",
    "from tokenizers.trainers import BpeTrainer \n",
    "from tokenizers.pre_tokenizers import Whitespace \n",
    "import os \n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0411c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bb51486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 36718\n",
      "})\n",
      "{'text': Value(dtype='string', id=None)}\n",
      "{'text': ' = Valkyria Chronicles III = \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets)\n",
    "\n",
    "train_split = raw_datasets['train']\n",
    "print(train_split)\n",
    "print(train_split.features)\n",
    "print(raw_datasets['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "254ef96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\", '', ' = = Gameplay = = \\n', '', \" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"]\n"
     ]
    }
   ],
   "source": [
    "def get_training_corpus():\n",
    "    batch_size = 1000 \n",
    "    for i in range(0, len(raw_datasets['train']), batch_size): \n",
    "        yield raw_datasets['train'][i: i + batch_size]['text']\n",
    "\n",
    "\n",
    "text_iterator = get_training_corpus()\n",
    "print(next(text_iterator)[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25f43586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# blank tokenizer with BPE model \n",
    "tokenizer = Tokenizer(BPE(unk_token='[UNK]'))\n",
    "\n",
    "# pre tokenizer which splits text into words \n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# vocab_size = # subword units the tokenizer can have \n",
    "trainer = BpeTrainer(vocab_size = 25000, special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]'])\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer = trainer)\n",
    "print('training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "624bfa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('custom_tokenizer', exist_ok = True)\n",
    "tokenizer.save('custom_tokenizer/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59a7188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence :  This is a test of our new tokenizer.\n",
      "tokens :  ['This', 'is', 'a', 'test', 'of', 'our', 'new', 'to', 'ken', 'izer', '.']\n",
      "token ids :  [1514, 1034, 69, 2319, 1031, 1165, 1366, 1036, 3201, 14114, 18]\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = Tokenizer.from_file('custom_tokenizer/tokenizer.json')\n",
    "\n",
    "sentence = \"This is a test of our new tokenizer.\"\n",
    "output = loaded_tokenizer.encode(sentence)\n",
    "print('sentence : ', sentence)\n",
    "print('tokens : ', output.tokens)\n",
    "print('token ids : ', output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a99802",
   "metadata": {},
   "source": [
    "### Finetuning GPT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dbb942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:5000]')\n",
    "\n",
    "dataset = dataset.filter(lambda ex: len(ex['text']) > 0)\n",
    "split = dataset.train_test_split(test_size = 0.1, seed = 42)\n",
    "train_raw = split['train']\n",
    "val_raw = split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25370b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old vocab size  50257\n",
      "new vocab size  50258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # tokenizer used for gpt2\n",
    "print('old vocab size ', len(tokenizer))\n",
    "# adding pad token for bacthing \n",
    "if tokenizer.pad_token is None: \n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})    \n",
    "\n",
    "print('new vocab size ', len(tokenizer))\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer)) # match new vocab size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bb4f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text \n",
    "max_length = 512 \n",
    "def tokenize_fn(examples): \n",
    "    return tokenizer(examples['text'], truncation=True, max_length=max_length)\n",
    "\n",
    "train_tok = train_raw.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "val_tok = val_raw.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "train_tok.set_format(type='torch', columns = ['input_ids', 'attention_mask'])\n",
    "val_tok.set_format(type='torch', columns = ['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9dcb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batches tokenizer data and pads sequence in the batch to the same length \n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False) # clm\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './gpt2-wikitext-finetuned', \n",
    "    num_train_epochs = 10, \n",
    "    per_device_train_batch_size = 4, \n",
    "    learning_rate = 5e-5, \n",
    "    weight_decay=0.01, \n",
    "    warmup_steps = 500, \n",
    "    eval_strategy = 'steps', \n",
    "    eval_steps = 500, \n",
    "    save_strategy = 'steps', \n",
    "    save_steps = 500, \n",
    "    load_best_model_at_end = True, \n",
    "    save_total_limit=3, \n",
    "    fp16 = True, # mix precision for speed \n",
    "    report_to = 'none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df5efa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = training_args, \n",
    "    train_dataset=train_tok, \n",
    "    eval_dataset=val_tok,  \n",
    "    data_collator=data_collator, \n",
    "    # processing_class = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "801415f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6032c1e42f134ddb986aba2aee837e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.1262, 'grad_norm': 6.765242099761963, 'learning_rate': 4.92e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e68548624940a88ec7860fff211ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.4197072982788086, 'eval_runtime': 16.9849, 'eval_samples_per_second': 19.017, 'eval_steps_per_second': 2.414, 'epoch': 0.69}\n",
      "{'loss': 3.3448, 'grad_norm': 6.078546524047852, 'learning_rate': 4.636094674556213e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0e9b98cb8146f886ca11c884f3797e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1987719535827637, 'eval_runtime': 15.4836, 'eval_samples_per_second': 20.861, 'eval_steps_per_second': 2.648, 'epoch': 1.38}\n",
      "{'loss': 3.1319, 'grad_norm': 4.227630138397217, 'learning_rate': 4.2662721893491124e-05, 'epoch': 2.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0611625b474002bd3de001c387ff24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1648731231689453, 'eval_runtime': 15.4974, 'eval_samples_per_second': 20.842, 'eval_steps_per_second': 2.646, 'epoch': 2.07}\n",
      "{'loss': 2.8552, 'grad_norm': 10.874784469604492, 'learning_rate': 3.896449704142012e-05, 'epoch': 2.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1bbbd5b99c430c8552044c4ba9c7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.135662078857422, 'eval_runtime': 15.5619, 'eval_samples_per_second': 20.756, 'eval_steps_per_second': 2.635, 'epoch': 2.75}\n",
      "{'loss': 2.6875, 'grad_norm': 5.875648021697998, 'learning_rate': 3.5266272189349114e-05, 'epoch': 3.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ae0c66f7c54a51b2f1260f4e526f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1644887924194336, 'eval_runtime': 15.5246, 'eval_samples_per_second': 20.806, 'eval_steps_per_second': 2.641, 'epoch': 3.44}\n",
      "{'loss': 2.5883, 'grad_norm': 6.053055286407471, 'learning_rate': 3.1568047337278106e-05, 'epoch': 4.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2db2b8176484c1c81fb2c6fd7e10425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1842610836029053, 'eval_runtime': 15.5, 'eval_samples_per_second': 20.839, 'eval_steps_per_second': 2.645, 'epoch': 4.13}\n",
      "{'loss': 2.4332, 'grad_norm': 8.307164192199707, 'learning_rate': 2.7869822485207102e-05, 'epoch': 4.82}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca935ab736c2459a82492176a4ca3475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1779539585113525, 'eval_runtime': 15.4646, 'eval_samples_per_second': 20.886, 'eval_steps_per_second': 2.651, 'epoch': 4.82}\n",
      "{'loss': 2.2933, 'grad_norm': 5.00571346282959, 'learning_rate': 2.4171597633136094e-05, 'epoch': 5.51}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b33800755747f89633fdaa85ced0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.221693992614746, 'eval_runtime': 15.4693, 'eval_samples_per_second': 20.88, 'eval_steps_per_second': 2.65, 'epoch': 5.51}\n",
      "{'loss': 2.236, 'grad_norm': 15.666465759277344, 'learning_rate': 2.047337278106509e-05, 'epoch': 6.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a30148a875e4d8f93875e53359562c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2799105644226074, 'eval_runtime': 15.4679, 'eval_samples_per_second': 20.882, 'eval_steps_per_second': 2.651, 'epoch': 6.2}\n",
      "{'loss': 2.1419, 'grad_norm': 6.070513725280762, 'learning_rate': 1.6775147928994085e-05, 'epoch': 6.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22cba0053274c388903e02ce72a6f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2689168453216553, 'eval_runtime': 15.7307, 'eval_samples_per_second': 20.533, 'eval_steps_per_second': 2.606, 'epoch': 6.89}\n",
      "{'loss': 2.0368, 'grad_norm': 6.930351734161377, 'learning_rate': 1.3076923076923078e-05, 'epoch': 7.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594eafdf32c84b51b54775b5eeb9f0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.315659523010254, 'eval_runtime': 16.3186, 'eval_samples_per_second': 19.793, 'eval_steps_per_second': 2.512, 'epoch': 7.58}\n",
      "{'loss': 2.0123, 'grad_norm': 8.146483421325684, 'learning_rate': 9.378698224852072e-06, 'epoch': 8.26}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bb38bcf53f44c2ac22d1ad419e39ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3545081615448, 'eval_runtime': 16.0478, 'eval_samples_per_second': 20.127, 'eval_steps_per_second': 2.555, 'epoch': 8.26}\n",
      "{'loss': 1.9532, 'grad_norm': 6.600475311279297, 'learning_rate': 5.680473372781065e-06, 'epoch': 8.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0a7378eadb4191a36dcb618b650034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3555212020874023, 'eval_runtime': 15.9897, 'eval_samples_per_second': 20.2, 'eval_steps_per_second': 2.564, 'epoch': 8.95}\n",
      "{'loss': 1.9068, 'grad_norm': 6.371081829071045, 'learning_rate': 1.9822485207100593e-06, 'epoch': 9.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c764133e797347b88d0a19078c761ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.386094093322754, 'eval_runtime': 15.7731, 'eval_samples_per_second': 20.478, 'eval_steps_per_second': 2.599, 'epoch': 9.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4944.6942, 'train_samples_per_second': 5.873, 'train_steps_per_second': 1.468, 'train_loss': 3.149932844514032, 'epoch': 10.0}\n",
      "82.0  m 27.058065599994734 secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print((end_time-start_time)//60, ' m', (end_time-start_time)%60, 'secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dbda366",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['attention_ids'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\ghora\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ghora\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1640\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1638\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[0;32m   1639\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1640\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model)\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghora\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1238\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m   1235\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[1;32m-> 1238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1241\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['attention_ids'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "prompt = \"Himalaya mountains are \"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors = 'pt', padding = True).to('cuda')\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids = encoding['input_ids'], \n",
    "    attention_ids = encoding['input_ids'], \n",
    "    attention_mask = encoding['attention_mask'], \n",
    "    max_length = 100, \n",
    "    num_return_sequences = 1, \n",
    "    temperature = 0.7, \n",
    "    top_k = 50, \n",
    "    repetition_penalty = 1.2, \n",
    "    do_sample = True \n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c1478",
   "metadata": {},
   "source": [
    "### Fine tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da2fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d0a1849c104463bac956f016420814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghora\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ghora\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ae13f6cf8944b184ab50426146e026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3324d27f6e20403f81007b19d698c4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9739e688f5c64c94987ec1483a7df9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fe17c4e1ea40598975ca2abdff089d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea232a3d5fe7471d830849ea634b0bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier\\'s plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it\\'s the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset('imdb')\n",
    "\n",
    "small_train_dataset = raw_datasets['train'].shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = raw_datasets['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "small_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f385c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017f1ca386014f36ba51af4177048b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghora\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ghora\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c12b9259704ce4958c7a851e33bd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693445f1f7ec4bef87481167cd0cd650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88794398ee0b4a14b2284ac529451071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509030a893ed40c19565a5250baea4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f7fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a3cf631d4248179776411283b02bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579a5148d1144674a2641623a4a4686c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example): \n",
    "    return tokenizer(example['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched = True)\n",
    "tokenized_test_dataset = small_test_dataset.map(tokenize_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd5777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4149a7b633a04778a3f4fa8ad1b0ac17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred): \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis = -1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='sentiment_model', \n",
    "    eval_strategy='epoch', \n",
    "    num_train_epochs=2, \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8, \n",
    "    learning_rate=5e-5, \n",
    "    weight_decay=0.01, \n",
    "    fp16=True, \n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fce14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d27a3498c854566ac0d76bcc17aa366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 149.0811, 'train_samples_per_second': 20.123, 'train_steps_per_second': 2.515, 'train_loss': 0.29058148193359373, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.29058148193359373, metrics={'train_runtime': 149.0811, 'train_samples_per_second': 20.123, 'train_steps_per_second': 2.515, 'total_flos': 397402195968000.0, 'train_loss': 0.29058148193359373, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    train_dataset=tokenized_train_dataset, \n",
    "    eval_dataset= tokenized_test_dataset, \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923718c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9904590845108032}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9930827617645264}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model = trainer.model, tokenizer=tokenizer, device = 'cuda')\n",
    "\n",
    "print(sentiment_pipeline('this movie was fantastic, really loved it'))\n",
    "print(sentiment_pipeline('this movie was boring, really hated it'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af3ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
